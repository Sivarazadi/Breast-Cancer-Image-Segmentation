{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd \n",
    "import imageio.v2 as imageio\n",
    "import cv2\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the current working directory to the path specified\n",
    "path = 'C:\\\\Users\\\\Sivar\\\\GitHub\\\\Breast_Cancer_Image_Segmentation\\\\data'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_images_and_masks(folder_path):\n",
    "  # Load the images and masks from the folder\n",
    "  images = [cv2.imread(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if f.endswith('_image.png')]\n",
    "  masks = [cv2.imread(os.path.join(folder_path, f), cv2.IMREAD_GRAYSCALE) for f in os.listdir(folder_path) if f.endswith('_mask.png')]\n",
    "\n",
    "  # Resize the images and masks to a desired size\n",
    "  desired_size = (256, 256)\n",
    "  images = [cv2.resize(image, desired_size) for image in images]\n",
    "  masks = [cv2.resize(mask, desired_size, interpolation=cv2.INTER_NEAREST) for mask in masks]\n",
    "\n",
    "  # Convert the images to RGB format (if they are not already)\n",
    "  images = [cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images]\n",
    "\n",
    "  # Normalize the pixel values of the images and masks\n",
    "  images = [image / 255.0 for image in images]\n",
    "  masks = [mask / 255.0 for mask in masks]\n",
    "\n",
    "  # Convert the images and masks to numpy arrays\n",
    "  images = np.array(images)\n",
    "  masks = np.array(masks)\n",
    "\n",
    "  return images, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the three folders\n",
    "benign_folder = 'C:\\\\Users\\\\Sivar\\\\GitHub\\\\Breast_Cancer_Image_Segmentation\\\\data\\\\benign'\n",
    "malignant_folder = 'C:\\\\Users\\\\Sivar\\\\GitHub\\\\Breast_Cancer_Image_Segmentation\\\\data\\\\malignant'\n",
    "normal_folder = 'C:\\\\Users\\\\Sivar\\\\GitHub\\\\Breast_Cancer_Image_Segmentation\\\\data\\\\normal'\n",
    "\n",
    "# Preprocess the images and masks from the benign, malignant, and normal folders\n",
    "benign_images, benign_masks = preprocess_images_and_masks(benign_folder)\n",
    "malignant_images, malignant_masks = preprocess_images_and_masks(malignant_folder)\n",
    "normal_images, normal_masks = preprocess_images_and_masks(normal_folder)\n",
    "\n",
    "# Concatenate the images and masks from the three folders\n",
    "images = np.concatenate((benign_images, malignant_images, normal_images), axis=0)\n",
    "masks = np.concatenate((benign_masks, malignant_masks, normal_masks), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the images and masks arrays\n",
    "if len(images) == 0 or len(masks) == 0:\n",
    "  print(\"Error: Images or masks array is empty\")\n",
    "elif len(images) != len(masks):\n",
    "  print(\"Error: Mismatch in number of images and masks\")\n",
    "else:\n",
    "  # Shuffle the images and masks together\n",
    "  combined = list(zip(images, masks))\n",
    "  np.random.shuffle(combined)\n",
    "  images, masks = zip(*combined)\n",
    "\n",
    "  # Convert the images and masks to numpy arrays\n",
    "  images = np.array(images)\n",
    "  masks = np.array(masks)\n",
    "\n",
    "  # Calculate the number of images in each split\n",
    "  num_images = len(images)\n",
    "  num_train = int(0.90 * num_images)\n",
    "  num_val = int(0.075 * num_images)\n",
    "  num_test = num_images - num_train - num_val\n",
    "\n",
    "  # Split the images and masks into training, validation, and test sets\n",
    "  train_images = images[:num_train]\n",
    "  train_masks = masks[:num_train]\n",
    "  val_images = images[num_train:num_train+num_val]\n",
    "  val_masks = masks[num_train:num_train+num_val]\n",
    "  test_images = images[num_train+num_val:]\n",
    "  test_masks = masks[num_train+num_val:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentation pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomHeight(factor=(0.1, 0.2)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomWidth(factor=(0.1, 0.2))\n",
    "])\n",
    "\n",
    "# Create a dataset with the data augmentation pipeline\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))\n",
    "dataset = dataset.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "# Configure the dataset for training\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print the first batch of augmented images and masks\n",
    "for x, y in dataset.take(1):\n",
    "    for i in range(len(x)):\n",
    "        print(f'Augmented image {i}:')\n",
    "        plt.imshow(x[i])\n",
    "        plt.show()\n",
    "        print(f'Augmented mask {i}:')\n",
    "        plt.imshow(y[i], cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Display of some image/mask outputs)\n",
    "\n",
    "Augmented Image 0: \n",
    "\n",
    "<img src='./images/aug_img_0.png' style='width:500px;height:400px;'>\n",
    "\n",
    "Augmented Mask 0:\n",
    "\n",
    "<img src='./images/aug_mask_0.png' style='width:500px;height:400px;'> \n",
    "\n",
    "Augmented Image 1: \n",
    "\n",
    "<img src='./images/aug_img_1.png' style='width:500px;height:400px;'>  \n",
    "\n",
    "Augmented Mask 1:\n",
    "\n",
    "<img src='./images/aug_mask_1.png' style='width:500px;height:400px;'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Conv2DTranspose, BatchNormalization, ReLU, Conv2D, UpSampling2D, MaxPool2D, Dropout\n",
    "from keras import Input, Model\n",
    "\n",
    "def create_upsample_layer(filters, kernel_size, padding='same', kernel_initializer='he_normal'):\n",
    "    \"\"\" This function creates a layer that upsamples an input tensor using a convolutional, batch\n",
    "    normalization, and ReLU activation, followed by an upsampling operation. \"\"\"\n",
    "    def layer(x):\n",
    "        x = Conv2D(filters, kernel_size, padding=padding, kernel_initializer=kernel_initializer)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "def create_downsample_layer(filters, kernel_size, padding='same', kernel_initializer='he_normal'):\n",
    "    \"\"\" This function creates a layer that downsamples an input tensor using a convolutional, batch\n",
    "    normalization, and ReLU activation, followed by a max pooling operation. \"\"\"\n",
    "    def layer(x):\n",
    "        x = Conv2D(filters, kernel_size, padding=padding, kernel_initializer=kernel_initializer)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "        x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "dropout_rate = 0.15\n",
    "\n",
    "# Example usage\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "# Encoder part of the DeepUNet\n",
    "x = create_downsample_layer(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "x = create_downsample_layer(128, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_downsample_layer(256, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_downsample_layer(512, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_downsample_layer(1024, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_downsample_layer(2048, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "\n",
    "# Decoder part of the DeepUNet\n",
    "x = create_upsample_layer(1024, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(512, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(256, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(128, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(64, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = create_upsample_layer(32, 3, padding='same', kernel_initializer='he_normal')(x)\n",
    "outputs = Conv2D(1, 1, padding = 'same', activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: \"model_3\"\n",
    "\n",
    "| Layer (type)  |  Output Shape   | Param # |\n",
    "|----------|----------|----------|\n",
    "| input_4 (InputLayer)  | [(None, 256, 256, 3)]  | 0   |\n",
    "| conv2d_39 (Conv2D)  | (None, 256, 256, 64)   | 1792   |\n",
    "| batch_normalization_36 (BatchNormalization)    | (None, 256, 256, 64)   | 256   |\n",
    "| re_lu_36 (ReLU)    | (None, 256, 256, 64)   | 0   |\n",
    "| max_pooling2d_18 (MaxPooling2D)   | (None, 128, 128, 64)   | 0   |\n",
    "| conv2d_40 (Conv2D)   | (None, 128, 128, 128)   | 73856   |\n",
    "| batch_normalization_37 (BatchNormalization)  | (None, 128, 128, 128)   |  512   |\n",
    "| re_lu_37 (ReLU)   | (None, 128, 128, 128)   | 0  |\n",
    "| max_pooling2d_19  (MaxPooling2D)  | (None, 64, 64, 128)   | 0   |\n",
    "| ...   |    |    |\n",
    "| conv2d_42 (Conv2D)   | (None, 32, 32, 512)   | 1180160   |\n",
    "| batch_normalization_39 (BatchNormalization)  | (None, 32, 32, 512)   | 2048   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define early stopping and reduce learning rate on plateau callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n",
    "                           mode='auto', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, \n",
    "                              verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    # Flatten the predictions and ground truth\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "    \n",
    "    # Compute the intersection and union\n",
    "    intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    union = tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat)\n",
    "    \n",
    "    # Compute the Dice loss\n",
    "    dice_loss = 1 - 2 * intersection / union\n",
    "    \n",
    "    return dice_loss\n",
    "\n",
    "# Compile the model with the Dice loss\n",
    "model.compile(loss=dice_loss, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs and the batch size\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# Train the UNet model on the training data\n",
    "history = model.fit(dataset, \n",
    "                    batch_size=batch_size, epochs=num_epochs,\n",
    "                    callbacks=[early_stop, reduce_lr],\n",
    "                    validation_data=(val_images, val_masks))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/50 \\\n",
    "22/22 [==============================] - 300s 13s/step - loss: 0.7448 - accuracy: 0.7145 - val_loss: 0.8166 - val_accuracy: 0.1092 - lr: 0.0010 \\\n",
    "Epoch 2/50 \\\n",
    "22/22 [==============================] - 264s 12s/step - loss: 0.6747 - accuracy: 0.8321 - val_loss: 0.8075 - val_accuracy: 0.1496 - lr: 0.0010 \\\n",
    "Epoch 3/50 \\\n",
    "22/22 [==============================] - 261s 12s/step - loss: 0.6309 - accuracy: 0.8832 - val_loss: 0.7907 - val_accuracy: 0.2538 - lr: 0.0010 \\\n",
    "Epoch 4/50 \\\n",
    "22/22 [==============================] - 260s 12s/step - loss: 0.5814 - accuracy: 0.9065 - val_loss: 0.6702 - val_accuracy: 0.7285 - lr: 0.0010 \\\n",
    "Epoch 5/50 \\\n",
    "22/22 [==============================] - 265s 12s/step - loss: 0.5450 - accuracy: 0.9211 - val_loss: 0.6946 - val_accuracy: 0.6409 - lr: 0.0010 \\\n",
    "Epoch 6/50 \\\n",
    "22/22 [==============================] - 265s 12s/step - loss: 0.4818 - accuracy: 0.9317 - val_loss: 0.5916 - val_accuracy: 0.7956 - lr: 0.0010 \\\n",
    "Epoch 7/50 \\\n",
    "22/22 [==============================] - 261s 12s/step - loss: 0.4092 - accuracy: 0.9476 - val_loss: 0.5503 - val_accuracy: 0.9095 - lr: 0.0010 \\\n",
    "... \\\n",
    "Epoch 49/50 \\\n",
    "22/22 [==============================] - 240s 11s/step - loss: 0.0918 - accuracy: 0.9875 - val_loss: 0.3449 - val_accuracy: 0.9462 - lr: 0.0010 \\\n",
    "Epoch 50/50 \\\n",
    "22/22 [==============================] - 252s 11s/step - loss: 0.0841 - accuracy: 0.9879 - val_loss: 0.2644 - val_accuracy: 0.9505 - lr: 0.0010 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs and corresponding val_loss and val_accuracy values\n",
    "epochs_list = list(range(1, len(history.history['val_loss']) + 1))\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot the epoch vs val_loss\n",
    "ax1 = plt.subplot(111)\n",
    "ax1.plot(epochs_list, val_accuracy, 'r-')\n",
    "ax1.set_ylabel('Validation Accuracy', color='r')\n",
    "\n",
    "# Create a second y-axis for the val_accuracy\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(epochs_list, val_loss, 'b-')\n",
    "ax2.set_ylabel('Validation Loss', color='b')\n",
    "\n",
    "# Set the same scaling on the y-axes\n",
    "ax1.set_ylim([0.1, 1])\n",
    "ax2.set_ylim([0.1, 1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/loss_acc.png' style='width:600px;height:400px;'>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,10):\n",
    "    # Select an image and its true mask\n",
    "    image = test_images[i]\n",
    "    mask = test_masks[i]\n",
    "\n",
    "    # Make a prediction using the model\n",
    "    prediction = model.predict(image[None, ...])[0]\n",
    "\n",
    "    # Display the image and the true mask\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title(\"Image\")\n",
    "    ax2.imshow(mask)\n",
    "    ax2.set_title(\"True Mask\")\n",
    "\n",
    "    # Display the image and the model's prediction\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title(\"Image\")\n",
    "    ax2.imshow(prediction)\n",
    "    ax2.set_title(\"Model Mask\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Display of some image/mask outputs)\n",
    "\n",
    "<img src='./images/true_mask_0.png' style='width:600px;height:300px;'> \n",
    "\n",
    "<img src='./images/model_mask_0.png' style='width:600px;height:300px;'> \n",
    "\n",
    "<img src='./images/true_mask_1.png' style='width:600px;height:300px;'> \n",
    "\n",
    "<img src='./images/model_mask_1.png' style='width:600px;height:300px;'>\n",
    "\n",
    "<img src='./images/true_mask_2.png' style='width:600px;height:300px;'> \n",
    "\n",
    "<img src='./images/model_mask_2.png' style='width:600px;height:300px;'> \n",
    "\n",
    "<img src='./images/true_mask_3.png' style='width:600px;height:300px;'> \n",
    "\n",
    "<img src='./images/model_mask_3.png' style='width:600px;height:300px;'>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few reasons why the model might perform poorly on images without cancer. One reason is that it simply hasn't been trained on enough examples of these types of images. This means that the model doesn't have enough information to accurately classify these images. Another reason could be that the model is overfitting to the images that do contain cancer, and is therefore not able to generalize well to images without cancer.\n",
    "\n",
    "One way to improve the performance on images without cancer would be to set a lower bound on the image segmentation. This would help the model to more accurately identify areas of the image that are likely to contain cancer, and would therefore improve its performance on images without cancer.\n",
    "\n",
    "This project could be revisited in the future to set a lower bound on the image segmentation and add more non-cancerous images/masks to see if it leads to improved performance on images without cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test images\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Compute the mean IoU metric on the test set\n",
    "mean_iou = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "mean_iou.update_state(test_masks, predictions)\n",
    "print(\"Mean IoU on test set: {:.3f}\".format(mean_iou.result()))\n",
    "\n",
    "# Compute the precision and recall on the test set\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "\n",
    "precision.update_state(test_masks, predictions)\n",
    "recall.update_state(test_masks, predictions)\n",
    "\n",
    "# Compute the F1 score on the test set\n",
    "f1_score = 2 * (precision.result() * recall.result()) / (precision.result() + recall.result())\n",
    "print(\"F1 score on test set: {:.3f}\".format(f1_score))\n",
    "\n",
    "# Reset the metrics\n",
    "mean_iou.reset_states()\n",
    "precision.reset_states()\n",
    "recall.reset_states()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/1 [==============================] - 1s 1s/step \\\n",
    "Mean IoU on test set: 0.735 \\\n",
    "F1 score on test set: 0.793"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d99d795a8355026be406cf19aa3922dd815f271261243126f21d0f482ffb091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
